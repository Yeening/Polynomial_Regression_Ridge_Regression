{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning HW2 Poly Regression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-514.9697362710486"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = load_data_set(\"dataPoly.txt\")\n",
    "normal_equation(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# Parse the file and return 2 numpy arrays \n",
    "def load_data_set(filename):\n",
    "    # your code\n",
    "    x = []\n",
    "    y = []\n",
    "    f = open(filename, 'r')\n",
    "    for line in f:\n",
    "        data = line.split()\n",
    "        x.append(data[0]) #elements before the last column\n",
    "        y.append(data[1]) #the last element\n",
    "    return np.array(x,dtype=float).reshape(-1,1), np.array(y,dtype=float).reshape(-1,1)\n",
    "#     return x, y\n",
    "# Find theta using the normal equation\n",
    "def normal_equation(x, y):\n",
    "    # your code\n",
    "    inv_xTx = np.linalg.inv(np.dot(x.T,x))\n",
    "    theta = inv_xTx.dot(x.T).dot(y).reshape(-1,1)\n",
    "    return theta[0][0]\n",
    "#     return theta.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-12-b4e74b0418ae>, line 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-b4e74b0418ae>\"\u001b[0;36m, line \u001b[0;32m40\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# Step 2: \n",
    "# Given a n by 1 dimensional array return an n by num_dimension array\n",
    "# consisting of [1, x, x^2, ...] in each row\n",
    "# x: input array with size n\n",
    "# degree: degree number, an int\n",
    "# result: polynomial basis based reformulation of x \n",
    "def increase_poly_order(x, degree):\n",
    "    # your code\n",
    "    return result\n",
    "\n",
    "# split the data into train and test examples by the train_proportion\n",
    "# i.e. if train_proportion = 0.8 then 80% of the examples are training and 20%\n",
    "# are testing\n",
    "def train_test_split(x, y, train_proportion):\n",
    "    # your code\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "# Find theta using the gradient descent/normal equation\n",
    "def solve_regression(x, y,method):\n",
    "    \n",
    "    if(method=='G'):\n",
    "        # your GD code from HW1 or better version that returns best theta as well as theta at each epoch \n",
    "    else:\n",
    "        thetas=[]\n",
    "        # return theta using normal equation\n",
    "        # no thetas for normal equation , return empty array \n",
    "    return theta, thetas \n",
    "\n",
    "# Given an array of y and y_predict return loss\n",
    "# y: an array of size n\n",
    "# y_predict: an array of size n\n",
    "# loss: a single float\n",
    "def get_loss(y, y_predict):\n",
    "    # your code\n",
    "    return loss\n",
    "\n",
    "# Given an array of x and theta predict y\n",
    "# x: an array with size n x d\n",
    "# theta: np array including parameters\n",
    "# y_predict: prediction labels, an array with size n\n",
    "def predict(x, theta):\n",
    "    # your code\n",
    "   return y_predict\n",
    "\n",
    "\n",
    "# Given a list of thetas one per (s)GD epoch\n",
    "# this creates a plot of epoch vs prediction loss (one about train, and another about test)\n",
    "# this figure checks GD optimization traits of the best theta \n",
    "def plot_epoch_losses(x_train, x_test, y_train, y_test, best_thetas, title):\n",
    "    # your code \n",
    "\n",
    "\n",
    "# Given a list of degrees.\n",
    "# For each degree in the list, train a polynomial regression.\n",
    "# Return training loss and validation loss for a polynomial regression of order degree for\n",
    "# each degree in degrees. \n",
    "# Use 60% training data and 20% validation data. Leave the last 20% for testing later.\n",
    "# Input:\n",
    "# x: an array with size n x d\n",
    "# y: an array with size n\n",
    "# degrees: A list of degrees\n",
    "# Output:\n",
    "# training_losses: a list of losses on the training dataset\n",
    "# validation_losses: a list of losses on the validation dataset\n",
    "def get_loss_per_poly_order(x, y, degrees):\n",
    "    # your code\n",
    "    return training_losses, validation_losses\n",
    "\n",
    "# Give the parameter theta, best-fit degree , plot the polynomial curve\n",
    "def best_fit_plot(theta, degree):\n",
    "    # your code\n",
    "\n",
    "\n",
    "def select_hyperparameter(degrees, x_train, x_test, y_train, y_test)\n",
    "    # Part 1: hyperparameter tuning:  \n",
    "    # Given a set of training examples, split it into train-validation splits\n",
    "    # do hyperparameter tune  \n",
    "    # come up with best model, then report error for best model\n",
    "    training_losses, validation_losses = get_loss_per_poly_order(x_train, y_train, degrees)\n",
    "    plt.plot(degrees, training_losses, label=\"training_loss\")\n",
    "    plt.plot(degrees, validation_losses, label=\"validation_loss\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(\"poly order vs validation_loss\")\n",
    "    plt.show()\n",
    "\n",
    "    # Part 2:  testing with the best learned theta \n",
    "    # Once the best hyperparameter has been chosen \n",
    "    # Train the model using that hyperparameter with all samples in the training \n",
    "    # Then use the test data to estimate how well this model generalizes.\n",
    "    best_degree = # fill in using best degree from part 2\n",
    "    x_train = increase_poly_order(x_train, best_degree) \n",
    "    best_theta, best_thetas = solve_regression(x_train, y_train,method)\n",
    "    best_fit_plot(best_theta, degree)\n",
    "    print(best_theta)\n",
    "    test_loss = get_loss(y_test, predict(x_test, best_theta))\n",
    "    train_loss = get_loss(y_train, predict(x_train, best_theta))    \n",
    "\n",
    "    # Part 3: visual analysis to check GD optimization traits of the best theta \n",
    "    plot_epoch_losses(x_train, x_test, y_train, y_test, best_thetas, \"best learned theta - train, test losses vs. GD epoch \")\n",
    "    return best_degree, best_theta, train_loss, test_loss\n",
    "\n",
    "\n",
    "# Given a list of dataset sizes [d_1, d_2, d_3 .. d_k]\n",
    "# Train a polynomial regression with first d_1, d_2, d_3, .. d_k samples\n",
    "# Each time, \n",
    "# return the a list of training and testing losses if we had that number of examples.\n",
    "# We are using 0.5 as the training proportion because it makes the testing_loss more stable\n",
    "# in reality we would use more of the data for training.\n",
    "# Input:\n",
    "# x: an array with size n x d\n",
    "# y: an array with size n\n",
    "# example_num: A list of dataset size\n",
    "# Output:\n",
    "# training_losses: a list of losses on the training dataset\n",
    "# testing_losses: a list of losses on the testing dataset\n",
    "def get_loss_per_tr_num_examples(x, y, example_num, train_proportion):\n",
    "    # your code\n",
    "    return training_losses, testing_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # select the best polynomial through train-validation-test formulation \n",
    "    x, y = load_data_set(\"dataPoly.txt\")\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, 0.8)\n",
    "    degrees = [i for i in range(10)]\n",
    "    best_degree, best_theta, train_loss, test_loss = select_hyperparameter(degrees, x_train, x_test, y_train, y_test)\n",
    " \n",
    "    # Part 4: analyze the effect of revising the size of train data: \n",
    "    # Show training error and testing error by varying the number for training samples \n",
    "    x, y = load_data_set(\"dataPoly.txt\")\n",
    "    x = increase_poly_order(x, 8)\n",
    "    example_num = [10*i for i in range(2, 11)] # python list comprehension\n",
    "    training_losses, testing_losses = get_loss_per_tr_num_examples(x, y, example_num, 0.5)\n",
    "    plt.plot(example_num, training_losses, label=\"training_loss\")\n",
    "    plt.plot(example_num, testing_losses, label=\"testing_losses\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(\"number of examples vs training_loss and testing_loss\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml3.7",
   "language": "python",
   "name": "ml3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
