{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning HW2 Poly Regression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = load_data_set(\"dataPoly.txt\")\n",
    "theta1 = normal_equation(x,y)\n",
    "theta = gradient_descent(x,y,0.0001,5000)[-1]\n",
    "predict(x,theta).shape\n",
    "# normal_equation(x,y).shape\n",
    "plot_epoch_losses(x, x, y, y, theta1, \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the file and return 2 numpy arrays \n",
    "def load_data_set(filename):\n",
    "    # your code\n",
    "    x = []\n",
    "    y = []\n",
    "    f = open(filename, 'r')\n",
    "    for line in f:\n",
    "        data = line.split()\n",
    "        x.append(data[0]) #elements before the last column\n",
    "        y.append(data[1]) #the last element\n",
    "    return np.array(x,dtype=float).reshape(-1,1), np.array(y,dtype=float).reshape(-1,1)\n",
    "#     return x, y\n",
    "# Find theta using the normal equation\n",
    "\n",
    "def normal_equation(x, y):\n",
    "    theta = np.linalg.inv(x.T.dot(x)).dot(x.T.dot(y))\n",
    "    return theta\n",
    "\n",
    "# Find thetas using gradient descent\n",
    "def gradient_descent(x, y, learning_rate, num_iterations):\n",
    "    # initialize theta as [1 1]\n",
    "#     theta = np.zeros(np.size(x, 1))\n",
    "    theta = np.zeros((1, 1))\n",
    "    thetas = []\n",
    "    for i in range(num_iterations):\n",
    "        loss = np.dot(x, theta) - y\n",
    "        gradient = np.dot(x.T,loss)\n",
    "        gradient /= len(x) # normalize by number of examples\n",
    "        theta = theta - learning_rate*gradient\n",
    "        thetas.append(theta)\n",
    "    return np.array(thetas)\n",
    "\n",
    "# Given an array of y and y_predict return loss\n",
    "# y: an array of size n\n",
    "# y_predict: an array of size n\n",
    "# loss: a single float\n",
    "def get_loss(y, y_predict):\n",
    "    # your code\n",
    "    loss = np.sum(np.square(y - y_predict)) / y.shape[0]\n",
    "    return loss\n",
    "\n",
    "# Given an array of x and theta predict y\n",
    "# x: an array with size n x d\n",
    "# theta: np array including parameters\n",
    "# y_predict: prediction labels, an array with size n\n",
    "def predict(x, theta):\n",
    "    # your code\n",
    "    y_predict = x.dot(theta)\n",
    "    return y_predict\n",
    "\n",
    "# Given a list of thetas one per (s)GD epoch\n",
    "# this creates plots of epoch vs prediction loss (one about train, and another about validation or test)\n",
    "# this figure checks GD optimization traits of the best theta \n",
    "def plot_epoch_losses(x_train, x_test, y_train, y_test, best_thetas, title):\n",
    "    losses = []\n",
    "    tslosses = []\n",
    "    epochs = []\n",
    "    epoch_num = 1\n",
    "    for theta in best_thetas:\n",
    "        tslosses.append(get_loss(y_train, predict(x_train, theta)))\n",
    "        losses.append(get_loss(y_test, predict(x_test, theta)))\n",
    "        epochs.append(epoch_num)\n",
    "        epoch_num += 1\n",
    "    plt.plot(epochs, losses, label=\"training_loss\")\n",
    "    plt.plot(epochs, tslosses, label=\"testing_loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-12-b4e74b0418ae>, line 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-b4e74b0418ae>\"\u001b[0;36m, line \u001b[0;32m40\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Given a list of thetas one per (s)GD epoch\n",
    "# this creates plots of epoch vs prediction loss (one about train, and another about validation or test)\n",
    "# this figure checks GD optimization traits of the best theta \n",
    "def plot_epoch_losses(x_train, x_test, y_train, y_test, best_thetas, title):\n",
    "    losses = []\n",
    "    tslosses = []\n",
    "    epochs = []\n",
    "    epoch_num = 1\n",
    "    for theta in best_thetas:\n",
    "        tslosses.append(get_loss(y_train, predict(x_train, theta)))\n",
    "        losses.append(get_loss(y_test, predict(x_test, theta)))\n",
    "        epochs.append(epoch_num)\n",
    "        epoch_num += 1\n",
    "    plt.plot(epochs, losses, label=\"training_loss\")\n",
    "    plt.plot(epochs, tslosses, label=\"testing_loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "##################################################################\n",
    "# the following functions are new for coding \n",
    "# 1. the train-validation based model selection \n",
    "# 2. the train+validate as train-whole  and the test split for model assessment \n",
    "# \n",
    "# \n",
    "# split the data into train and test examples by the train_proportion\n",
    "# i.e. if train_proportion = 0.8 then 80% of the examples are training and 20%\n",
    "# are testing\n",
    "def train_test_split(x, y, train_proportion):\n",
    "    # your code\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "##################################################################\n",
    "# the following functions are new for polynomial regression  \n",
    "# \n",
    "# Given a n by 1 dimensional array return an n by num_dimension array\n",
    "# consisting of [1, x, x^2, ...] in each row\n",
    "# x: input array with size n\n",
    "# degree: degree number, an int\n",
    "# result: polynomial basis based reformulation of x \n",
    "def increase_poly_order(x, degree):\n",
    "    # your code\n",
    "    return result\n",
    "\n",
    "# Give the parameter theta, best-fit degree , plot the polynomial curve\n",
    "def best_fit_plot(x, y, theta, degree):\n",
    "    plt.scatter(x, y)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "\n",
    "    #Calculates the x and y value for x evenly spaced 100 times between 0 and 10. Then uses these to plot the theta line\n",
    "    x = np.linspace(-2,2,100)\n",
    "    xp = increase_poly_order(x, degree)\n",
    "    y_predict = predict(xp, theta)\n",
    "    plt.plot(x, y_predict)\n",
    "    plt.title(\"data vs fitted polynomial curve\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################################################\n",
    "# the following functions are new for coding model selection for poly regression\n",
    "#\n",
    "# Given a list of degrees.\n",
    "# For each degree in the list, train a polynomial regression.\n",
    "# Return training loss and validation loss for a polynomial regression of order degree for\n",
    "# each degree in degrees. \n",
    "# Use 60% training data and 20% validation data. Leave the last 20% for testing later.\n",
    "# Input:\n",
    "# x: an array with size n x d\n",
    "# y: an array with size n\n",
    "# degrees: A list of degrees\n",
    "# Output:\n",
    "# training_losses: a list of losses on the training dataset\n",
    "# validation_losses: a list of losses on the validation dataset\n",
    "def get_loss_per_poly_order(x, y, degrees):\n",
    "    # your code\n",
    "    return training_losses, validation_losses\n",
    "\n",
    "\n",
    "def select_hyperparameter(degrees, x_train, y_train):\n",
    "    # Part 1: hyperparameter tuning:  \n",
    "    # Given a set of training examples, split it into train-validation splits\n",
    "    # do hyperparameter tune  \n",
    "    # come up with best model, then report error for best model\n",
    "    training_losses, validation_losses = get_loss_per_poly_order(x_train, y_train, degrees)\n",
    "    plt.plot(degrees, training_losses, label=\"training_loss\")\n",
    "    plt.plot(degrees, validation_losses, label=\"validation_loss\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(\"poly order vs validation_loss\")\n",
    "    plt.show()\n",
    "\n",
    "    # Part 2:  testing with the best learned theta \n",
    "    # Once the best hyperparameter has been chosen \n",
    "    # Train the model using that hyperparameter with all samples in the training \n",
    "    # Then use the test data to estimate how well this model generalizes.\n",
    "    #print(validation_losses)\n",
    "    valiFind = np.asarray(validation_losses)\n",
    "    findd = np.where(valiFind == np.amin(valiFind))\n",
    "    best_degree = degrees[int(findd[0])]\n",
    "    print(best_degree)\n",
    "\n",
    "    x_train_p = increase_poly_order(x_train, best_degree) \n",
    "    best_theta = normal_equation(x_train_p, y_train)\n",
    "    print(best_theta)\n",
    "    return best_degree, best_theta\n",
    "\n",
    "\n",
    "##################################################################\n",
    "# the following functions are new for coding model selection for poly regression\n",
    "# \n",
    "# Given a list of dataset sizes [d_1, d_2, d_3 .. d_k]\n",
    "# Train a polynomial regression with first d_1, d_2, d_3, .. d_k samples\n",
    "# Each time, \n",
    "# return the a list of training and testing losses if we had that number of examples.\n",
    "# We are using 0.5 as the training proportion because it makes the testing_loss more stable\n",
    "# in reality we would use more of the data for training.\n",
    "# Input:\n",
    "# x: an array with size n x d\n",
    "# y: an array with size n\n",
    "# example_num: A list of dataset size\n",
    "# Output:\n",
    "# training_losses: a list of losses on the training dataset\n",
    "# testing_losses: a list of losses on the testing dataset\n",
    "#\n",
    "# Given a list of sizes return the training and testing loss \n",
    "# when using the given series number of examples.\n",
    "def get_loss_per_num_examples(x, y, example_num, train_proportion):\n",
    "    training_losses = []\n",
    "    testing_losses = []\n",
    "    for i in example_num:\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x[:i], y[:i], 0.5)\n",
    "        theta = normal_equation(x_train, y_train)\n",
    "\n",
    "        training_losses.append(get_loss(y_train, predict(x_train, theta)))\n",
    "        testing_losses.append(get_loss(y_test, predict(x_test, theta)))\n",
    "    return training_losses, testing_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    ##################################################################\n",
    "    # Part 1: readin dataset / train , test split , leave test set not touch during model selection\n",
    "    # later select the best polynomial through train-validation-test formulation \n",
    "    x, y = load_data_set(\"dataPoly.txt\")\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, 0.8)\n",
    "    plot(x_train, y_train)\n",
    "    plot(x_test, y_test)\n",
    "\n",
    "\n",
    "    ##################################################################\n",
    "    # Part 2: select best_degree and derive its best_theta \n",
    "    # Given a set of training examples, split it into train-validation splits\n",
    "    # do hyperparameter tune / model selection  \n",
    "    # come up with best hyperparameter, then train on full (train+valid) to get the best parameter\n",
    "    degrees = [i for i in range(10)]\n",
    "    best_degree, best_theta = select_hyperparameter(degrees, x_train, y_train)\n",
    "    best_fit_plot(x_train, y_train, best_theta, best_degree)\n",
    "    best_fit_plot(x_test, y_test, best_theta, best_degree)\n",
    "    print(best_theta)\n",
    "\n",
    "    # model assessement to get the test loss \n",
    "    x_testp = increase_poly_order(x_test, best_degree)\n",
    "    print(get_loss(y_test, predict(x_testp, best_theta)))\n",
    "\n",
    " \n",
    "    ##################################################################\n",
    "    # Part 3: visual analysis to check GD optimization traits of the best theta\n",
    "    print(best_degree)\n",
    "    x_train_p = increase_poly_order(x_train, best_degree) \n",
    "    x_test_p = increase_poly_order(x_test, best_degree)\n",
    "    gbest_thetas = gradient_descent(x_train_p, y_train, 0.005, 2000) \n",
    "\n",
    "    best_fit_plot(x_train, y_train, gbest_thetas[-1], best_degree)\n",
    "    plot_epoch_losses(x_train_p, x_test_p, y_train, y_test, gbest_thetas, \"best learned theta - train, test losses vs. GD epoch \")\n",
    "\n",
    "\n",
    "    ##################################################################\n",
    "    # Part 4: analyze the effect of revising the size of train data: \n",
    "    # Show training error and testing error by varying the number for training samples \n",
    "    x, y = load_data_set(\"dataPoly.txt\")\n",
    "    x = increase_poly_order(x, 8)\n",
    "    example_num = [10*i for i in range(2, 21)] # python list comprehension\n",
    "    training_losses, testing_losses = get_loss_per_num_examples(x, y, example_num, 0.5)\n",
    "\n",
    "    plt.plot(example_num, training_losses, label=\"training_loss\")\n",
    "    plt.plot(example_num, testing_losses, label=\"test_losses\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(\"number of training examples vs training_loss and testing_loss\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml3.7",
   "language": "python",
   "name": "ml3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
